"""
ç¨ å¯†æ„ŸçŸ¥å¿«æ”¾ç®—æ³• (Density-Aware Speed-Up Algorithm)
æ ¸å¿ƒç®—æ³•æ¨¡å—

æœ¬é¡¹ç›®ä»£ç ç”±Manus AIå®Œæˆã€‚
"""

import torch
import numpy as np
from pydub import AudioSegment
import pyrubberband as pyrb
from typing import List, Dict
import os
import time

# ==============================================================================
# 1. ç²¾ç»†åŒ–VADï¼šè·å–è¯­éŸ³æ¦‚ç‡åºåˆ—
# ==============================================================================

def get_speech_probabilities(
    audio_path: str,
    vad_model,
    sampling_rate: int = 16000
) -> List[Dict[str, float]]:
    """
    ä½¿ç”¨Silero-VADæ¨¡å‹ï¼Œè·å–æ¯ä¸ªéŸ³é¢‘å—çš„è¯­éŸ³æ¦‚ç‡ã€‚

    Args:
        audio_path (str): è¾“å…¥éŸ³é¢‘æ–‡ä»¶è·¯å¾„ã€‚
        vad_model: åŠ è½½çš„Silero-VADæ¨¡å‹ã€‚
        sampling_rate (int): VADæ¨¡å‹æœŸæœ›çš„é‡‡æ ·ç‡ã€‚

    Returns:
        List[Dict[str, float]]: ä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«å—çš„'start'/'end'ï¼ˆæ¯«ç§’ï¼‰å’Œ'prob'ï¼ˆè¯­éŸ³æ¦‚ç‡ï¼‰ã€‚
    """
    try:
        audio = AudioSegment.from_file(audio_path)
        audio = audio.set_frame_rate(sampling_rate).set_channels(1)
    except Exception as e:
        print(f"åŠ è½½éŸ³é¢‘æ—¶å‡ºé”™: {e}")
        return []

    audio_samples = np.array(audio.get_array_of_samples()).astype(np.float32) / (2**15)
    
    # VADæ¨¡å‹æœŸæœ›çš„å—å¤§å°ï¼ˆä»¥æ ·æœ¬ä¸ºå•ä½ï¼‰
    # Silero-VAD åœ¨16kHzé‡‡æ ·ç‡ä¸‹æœŸæœ› 512 samples
    chunk_size_samples = 512
    
    probabilities = []
    
    for i in range(0, len(audio_samples), chunk_size_samples):
        chunk = audio_samples[i: i + chunk_size_samples]
        if len(chunk) < chunk_size_samples:
            padding = np.zeros(chunk_size_samples - len(chunk), dtype=np.float32)
            chunk = np.concatenate([chunk, padding])
            
        speech_prob = vad_model(torch.from_numpy(chunk), sampling_rate).item()
        
        start_ms = (i / sampling_rate) * 1000
        end_ms = ((i + len(chunk)) / sampling_rate) * 1000
        
        probabilities.append({'start': start_ms, 'end': end_ms, 'prob': speech_prob})

    print(f"VADæ¦‚ç‡åˆ†æå®Œæˆï¼Œç”Ÿæˆäº† {len(probabilities)} ä¸ªæ¦‚ç‡å—ã€‚")
    return probabilities

# ==============================================================================
# 2. ç”Ÿæˆå¤šçº§å¤„ç†ç‰‡æ®µåˆ—è¡¨
# ==============================================================================

def create_multi_level_segments(
    total_duration_ms: int,
    speech_probs: List[Dict[str, float]],
    silence_thresh: float,
    low_density_thresh: float
) -> List[Dict]:
    """
    æ ¹æ®è¯­éŸ³æ¦‚ç‡ï¼Œå°†éŸ³é¢‘åˆ’åˆ†ä¸º"é«˜å¯†åº¦"ã€"ä½å¯†åº¦"å’Œ"é™éŸ³"ç‰‡æ®µã€‚

    Args:
        total_duration_ms (int): éŸ³é¢‘æ€»æ—¶é•¿ï¼ˆæ¯«ç§’ï¼‰ã€‚
        speech_probs (List[Dict[str, float]]): VADæ¦‚ç‡å—åˆ—è¡¨ã€‚
        silence_thresh (float): ä½äºæ­¤æ¦‚ç‡çš„ä¸ºé™éŸ³ã€‚
        low_density_thresh (float): ä½äºæ­¤æ¦‚ç‡çš„ä¸ºä½å¯†åº¦è¯­éŸ³ã€‚

    Returns:
        List[Dict]: åŒ…å«æ‰€æœ‰ç‰‡æ®µä¿¡æ¯ï¼ˆç±»å‹ã€å¼€å§‹ã€ç»“æŸæ—¶é—´ï¼‰çš„åˆ—è¡¨ã€‚
    """
    segments = []
    if not speech_probs:
        return []

    current_segment_type = None
    current_segment_start = 0

    for prob_chunk in speech_probs:
        prob = prob_chunk['prob']
        
        if prob < silence_thresh:
            segment_type = 'silence'
        elif prob < low_density_thresh:
            segment_type = 'low_density_speech'
        else:
            segment_type = 'high_density_speech'
            
        if current_segment_type is None:
            current_segment_type = segment_type

        if segment_type != current_segment_type:
            segments.append({
                'type': current_segment_type,
                'start': current_segment_start,
                'end': prob_chunk['start']
            })
            current_segment_start = prob_chunk['start']
            current_segment_type = segment_type
    
    # æ·»åŠ æœ€åä¸€ä¸ªç‰‡æ®µ
    if current_segment_type is not None:
        segments.append({
            'type': current_segment_type,
            'start': current_segment_start,
            'end': total_duration_ms
        })
    
    print(f"éŸ³é¢‘è¢«åˆ’åˆ†ä¸º {len(segments)} ä¸ªå¤šçº§ç‰‡æ®µã€‚")
    return segments

# ==============================================================================
# 3. æ ¸å¿ƒç®—æ³• v2ï¼šå¸¦è¯­éŸ³å¯†åº¦æ„ŸçŸ¥çš„éçº¿æ€§å˜é€Ÿ
# ==============================================================================

def intelligent_speed_up_v2(
    audio_path: str,
    output_path: str,
    base_rate: float,
    high_density_factor: float,
    low_density_factor: float,
    silence_threshold: float = 0.2,
    low_density_threshold: float = 0.7
):
    """
    å¯¹éŸ³é¢‘æ–‡ä»¶è¿›è¡Œå¸¦è¯­éŸ³å¯†åº¦æ„ŸçŸ¥çš„æ™ºèƒ½å˜é€Ÿå¤„ç†ã€‚

    Args:
        audio_path (str): è¾“å…¥éŸ³é¢‘æ–‡ä»¶è·¯å¾„ã€‚
        output_path (str): è¾“å‡ºæ–‡ä»¶è·¯å¾„ã€‚
        base_rate (float): åŸºå‡†å€é€Ÿã€‚
        high_density_factor (float): é«˜å¯†åº¦è¯­éŸ³çš„é€Ÿåº¦è°ƒèŠ‚å› å­ã€‚
        low_density_factor (float): ä½å¯†åº¦è¯­éŸ³çš„é€Ÿåº¦è°ƒèŠ‚å› å­ã€‚
        silence_threshold (float): VADæ¦‚ç‡ä½äºæ­¤å€¼ä¸ºé™éŸ³ã€‚
        low_density_threshold (float): VADæ¦‚ç‡ä½äºæ­¤å€¼ä¸ºä½å¯†åº¦è¯­éŸ³ã€‚
    """
    start_time = time.time()
    print("å¼€å§‹åŠ è½½æ¨¡å‹å’ŒéŸ³é¢‘ (v2)...")

    try:
        model, _ = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=False)
        audio = AudioSegment.from_file(audio_path)
    except Exception as e:
        print(f"åˆå§‹åŒ–å¤±è´¥: {e}")
        raise
        
    audio = audio.set_frame_rate(16000).set_channels(1)
    total_duration_ms = len(audio)
    print(f"éŸ³é¢‘åŠ è½½å®Œæˆï¼Œæ€»æ—¶é•¿: {total_duration_ms / 1000:.2f} ç§’ã€‚")

    print("\n--- æ­¥éª¤ 1: VADæ¦‚ç‡åˆ†æ ---")
    speech_probabilities = get_speech_probabilities(audio_path, model)
    if not speech_probabilities:
        print("æ— æ³•åˆ†æéŸ³é¢‘æ¦‚ç‡ï¼Œå·²é€€å‡ºã€‚")
        raise ValueError("VADåˆ†æå¤±è´¥")

    print("\n--- æ­¥éª¤ 2: è®¡ç®—å¤šçº§å˜é€Ÿé€Ÿç‡ ---")
    segments = create_multi_level_segments(total_duration_ms, speech_probabilities, silence_threshold, low_density_threshold)
    
    durations = {'silence': 0, 'low_density_speech': 0, 'high_density_speech': 0}
    for s in segments:
        durations[s['type']] += (s['end'] - s['start'])

    # è®¡ç®—å„ç±»å‹è¯­éŸ³çš„é€Ÿåº¦
    speed_high = base_rate * high_density_factor
    speed_low = base_rate * low_density_factor
    
    # æ ¹æ®æ€»æ—¶é•¿çº¦æŸï¼Œåæ¨é™éŸ³é€Ÿåº¦
    target_total_duration = total_duration_ms / base_rate
    duration_after_speech_processing = (durations['high_density_speech'] / speed_high) + \
                                       (durations['low_density_speech'] / speed_low)
    
    remaining_duration_for_silence = target_total_duration - duration_after_speech_processing
    
    if remaining_duration_for_silence <= 0 or durations['silence'] == 0:
        speed_silence = 100.0
        print("è­¦å‘Š: äººå£°éƒ¨åˆ†å˜é€Ÿåå·²è¶…è¿‡ç›®æ ‡æ€»æ—¶é•¿ï¼Œé™éŸ³å°†è¢«æåº¦å‹ç¼©ã€‚")
    else:
        speed_silence = durations['silence'] / remaining_duration_for_silence

    speeds = {
        'high_density_speech': speed_high,
        'low_density_speech': speed_low,
        'silence': speed_silence
    }
    
    print(f"åŸºå‡†å€é€Ÿ: {base_rate:.2f}x")
    print(f"é«˜å¯†åº¦è¯­éŸ³é€Ÿåº¦: {speed_high:.2f}x (å› å­: {high_density_factor})")
    print(f"ä½å¯†åº¦è¯­éŸ³é€Ÿåº¦: {speed_low:.2f}x (å› å­: {low_density_factor})")
    print(f"è®¡ç®—å¾—å‡ºçš„é™éŸ³é€Ÿåº¦: {speed_silence:.2f}x")

    print("\n--- æ­¥éª¤ 3: åˆ†æ®µå˜é€Ÿä¸åˆå¹¶ ---")
    processed_segments = []
    for i, segment_info in enumerate(segments):
        start_ms, end_ms = segment_info['start'], segment_info['end']
        seg_type = segment_info['type']
        
        if start_ms >= end_ms: 
            continue

        original_chunk = audio[start_ms:end_ms]
        
        dtype = getattr(np, f"int{original_chunk.sample_width * 8}")
        samples = np.array(original_chunk.get_array_of_samples(), dtype=dtype)

        speed = speeds[seg_type]
        
        if abs(speed - 1.0) < 0.01:
            processed_chunk_samples = samples
        else:
            processed_chunk_samples = pyrb.time_stretch(samples, original_chunk.frame_rate, speed)

        processed_chunk_samples = (processed_chunk_samples * (2**15)).astype(dtype)
        
        processed_chunk = AudioSegment(
            processed_chunk_samples.tobytes(),
            frame_rate=original_chunk.frame_rate,
            sample_width=original_chunk.sample_width,
            channels=original_chunk.channels
        )
        processed_segments.append(processed_chunk)
        print(f"å¤„ç†ç‰‡æ®µ {i+1}/{len(segments)}: ç±»å‹={seg_type}, é€Ÿåº¦={speed:.2f}x, "
              f"åŸå§‹æ—¶é•¿={len(original_chunk)/1000:.2f}s, "
              f"å¤„ç†åæ—¶é•¿={len(processed_chunk)/1000:.2f}s")

    final_audio = sum(processed_segments, AudioSegment.empty())
    
    print("\n--- æ­¥éª¤ 4: å¯¼å‡ºæœ€ç»ˆéŸ³é¢‘ ---")
    final_audio.export(output_path, format=os.path.splitext(output_path)[1][1:])
    
    end_time = time.time()
    original_duration = total_duration_ms / 1000
    final_duration = len(final_audio) / 1000
    
    print("\nğŸ‰ å¤„ç†å®Œæˆ (v2)ï¼")
    print(f"æ–‡ä»¶å·²ä¿å­˜è‡³: {output_path}")
    print(f"æ€»è€—æ—¶: {end_time - start_time:.2f} ç§’")
    print(f"åŸå§‹éŸ³é¢‘æ—¶é•¿: {original_duration:.2f} ç§’")
    print(f"è¾“å‡ºéŸ³é¢‘æ—¶é•¿: {final_duration:.2f} ç§’")
    print(f"å®ç°çš„å¹³å‡å€é€Ÿ: {original_duration / final_duration:.2f}x (ç›®æ ‡: {base_rate:.2f}x)")
