"""
ç¨ å¯†æ„ŸçŸ¥å¿«æ”¾ç®—æ³• (Density-Aware Speed-Up Algorithm)
æ ¸å¿ƒç®—æ³•æ¨¡å—

æœ¬é¡¹ç›®ä»£ç ç”±Manus AIå®Œæˆã€‚
"""

import torch
import numpy as np
from pydub import AudioSegment
import pyrubberband as pyrb
from typing import List, Dict, Optional, Tuple
import os
import time

# ==============================================================================
# 1. ç²¾ç»†åŒ–VADï¼šè·å–è¯­éŸ³æ¦‚ç‡åºåˆ—
# ==============================================================================

def get_speech_probabilities(
    audio_path: str,
    vad_model,
    sampling_rate: int = 16000
) -> List[Dict[str, float]]:
    """
    ä½¿ç”¨Silero-VADæ¨¡å‹ï¼Œè·å–æ¯ä¸ªéŸ³é¢‘å—çš„è¯­éŸ³æ¦‚ç‡ã€‚

    Args:
        audio_path (str): è¾“å…¥éŸ³é¢‘æ–‡ä»¶è·¯å¾„ã€‚
        vad_model: åŠ è½½çš„Silero-VADæ¨¡å‹ã€‚
        sampling_rate (int): VADæ¨¡å‹æœŸæœ›çš„é‡‡æ ·ç‡ã€‚

    Returns:
        List[Dict[str, float]]: ä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«å—çš„'start'/'end'ï¼ˆæ¯«ç§’ï¼‰å’Œ'prob'ï¼ˆè¯­éŸ³æ¦‚ç‡ï¼‰ã€‚
    """
    try:
        audio = AudioSegment.from_file(audio_path)
        audio = audio.set_frame_rate(sampling_rate).set_channels(1)
    except Exception as e:
        print(f"åŠ è½½éŸ³é¢‘æ—¶å‡ºé”™: {e}")
        return []

    audio_samples = np.array(audio.get_array_of_samples()).astype(np.float32) / (2**15)
    
    # VADæ¨¡å‹æœŸæœ›çš„å—å¤§å°ï¼ˆä»¥æ ·æœ¬ä¸ºå•ä½ï¼‰
    # Silero-VAD åœ¨16kHzé‡‡æ ·ç‡ä¸‹æœŸæœ› 512 samples
    chunk_size_samples = 512
    
    probabilities = []
    
    for i in range(0, len(audio_samples), chunk_size_samples):
        chunk = audio_samples[i: i + chunk_size_samples]
        if len(chunk) < chunk_size_samples:
            padding = np.zeros(chunk_size_samples - len(chunk), dtype=np.float32)
            chunk = np.concatenate([chunk, padding])
            
        speech_prob = vad_model(torch.from_numpy(chunk), sampling_rate).item()
        
        start_ms = (i / sampling_rate) * 1000
        end_ms = ((i + len(chunk)) / sampling_rate) * 1000
        
        probabilities.append({'start': start_ms, 'end': end_ms, 'prob': speech_prob})

    print(f"VADæ¦‚ç‡åˆ†æå®Œæˆï¼Œç”Ÿæˆäº† {len(probabilities)} ä¸ªæ¦‚ç‡å—ã€‚")
    return probabilities

# ==============================================================================
# 2. ç”Ÿæˆå¤šçº§å¤„ç†ç‰‡æ®µåˆ—è¡¨
# ==============================================================================

def create_multi_level_segments(
    total_duration_ms: int,
    speech_probs: List[Dict[str, float]],
    silence_thresh: float,
    low_density_thresh: float
) -> List[Dict]:
    """
    æ ¹æ®è¯­éŸ³æ¦‚ç‡ï¼Œå°†éŸ³é¢‘åˆ’åˆ†ä¸º"é«˜å¯†åº¦"ã€"ä½å¯†åº¦"å’Œ"é™éŸ³"ç‰‡æ®µã€‚

    Args:
        total_duration_ms (int): éŸ³é¢‘æ€»æ—¶é•¿ï¼ˆæ¯«ç§’ï¼‰ã€‚
        speech_probs (List[Dict[str, float]]): VADæ¦‚ç‡å—åˆ—è¡¨ã€‚
        silence_thresh (float): ä½äºæ­¤æ¦‚ç‡çš„ä¸ºé™éŸ³ã€‚
        low_density_thresh (float): ä½äºæ­¤æ¦‚ç‡çš„ä¸ºä½å¯†åº¦è¯­éŸ³ã€‚

    Returns:
        List[Dict]: åŒ…å«æ‰€æœ‰ç‰‡æ®µä¿¡æ¯ï¼ˆç±»å‹ã€å¼€å§‹ã€ç»“æŸæ—¶é—´ï¼‰çš„åˆ—è¡¨ã€‚
    """
    segments = []
    if not speech_probs:
        return []

    current_segment_type = None
    current_segment_start = 0

    for prob_chunk in speech_probs:
        prob = prob_chunk['prob']
        
        if prob < silence_thresh:
            segment_type = 'silence'
        elif prob < low_density_thresh:
            segment_type = 'low_density_speech'
        else:
            segment_type = 'high_density_speech'
            
        if current_segment_type is None:
            current_segment_type = segment_type

        if segment_type != current_segment_type:
            segments.append({
                'type': current_segment_type,
                'start': current_segment_start,
                'end': prob_chunk['start']
            })
            current_segment_start = prob_chunk['start']
            current_segment_type = segment_type
    
    # æ·»åŠ æœ€åä¸€ä¸ªç‰‡æ®µ
    if current_segment_type is not None:
        segments.append({
            'type': current_segment_type,
            'start': current_segment_start,
            'end': total_duration_ms
        })
    
    print(f"éŸ³é¢‘è¢«åˆ’åˆ†ä¸º {len(segments)} ä¸ªå¤šçº§ç‰‡æ®µã€‚")
    return segments

# ==============================================================================
# 3. éŸ³é¢‘è´¨é‡å‚æ•°ç±»
# ==============================================================================

class AudioQualityConfig:
    """éŸ³é¢‘è´¨é‡é…ç½®ç±»"""
    
    def __init__(
        self,
        sample_rate: Optional[int] = None,
        bit_depth: Optional[int] = None,
        output_format: str = "mp3",
        mp3_bitrate: str = "192k",
        preserve_channels: bool = True
    ):
        """
        åˆå§‹åŒ–éŸ³é¢‘è´¨é‡é…ç½®
        
        Args:
            sample_rate: é‡‡æ ·ç‡ï¼ˆHzï¼‰ï¼ŒNone è¡¨ç¤ºä¿æŒåŸå§‹
            bit_depth: ä½æ·±ï¼ˆ8, 16, 24, 32ï¼‰ï¼ŒNone è¡¨ç¤ºä¿æŒåŸå§‹
            output_format: è¾“å‡ºæ ¼å¼ ("mp3", "wav", "flac", "ogg")
            mp3_bitrate: MP3 ç ç‡ ("128k", "192k", "256k", "320k")
            preserve_channels: æ˜¯å¦ä¿æŒåŸå§‹å£°é“æ•°
        """
        self.sample_rate = sample_rate
        self.bit_depth = bit_depth
        self.output_format = output_format.lower()
        self.mp3_bitrate = mp3_bitrate
        self.preserve_channels = preserve_channels
    
    @classmethod
    def quick_preview(cls) -> 'AudioQualityConfig':
        """å¿«é€Ÿé¢„è§ˆæ¨¡å¼ï¼š16kHz MP3 128kï¼Œä¸ä¿æŒå£°é“"""
        return cls(
            sample_rate=16000,
            bit_depth=16,
            output_format="mp3",
            mp3_bitrate="128k",
            preserve_channels=False
        )
    
    @classmethod
    def from_source(cls, audio_path: str) -> 'AudioQualityConfig':
        """ä»æºæ–‡ä»¶è·å–é…ç½®ï¼Œè¾“å‡º WAV æ— æŸæ ¼å¼ï¼Œä¿æŒå£°é“"""
        audio = AudioSegment.from_file(audio_path)
        return cls(
            sample_rate=audio.frame_rate,
            bit_depth=audio.sample_width * 8,
            output_format="wav",
            mp3_bitrate="320k",
            preserve_channels=True
        )
    
    def __repr__(self):
        return f"AudioQualityConfig(sample_rate={self.sample_rate}, bit_depth={self.bit_depth}, format={self.output_format}, mp3_bitrate={self.mp3_bitrate}, preserve_channels={self.preserve_channels})"


def get_audio_info(audio_path: str) -> Dict:
    """
    è·å–éŸ³é¢‘æ–‡ä»¶çš„è¯¦ç»†ä¿¡æ¯
    
    Args:
        audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
    
    Returns:
        åŒ…å«éŸ³é¢‘ä¿¡æ¯çš„å­—å…¸
    """
    audio = AudioSegment.from_file(audio_path)
    
    # è·å–æ–‡ä»¶æ ¼å¼
    ext = os.path.splitext(audio_path)[1].lower().lstrip('.')
    if ext in ['mp3', 'wav', 'flac', 'ogg', 'm4a', 'aac']:
        file_format = ext
    else:
        file_format = 'unknown'
    
    return {
        'duration_ms': len(audio),
        'duration_seconds': len(audio) / 1000.0,
        'sample_rate': audio.frame_rate,
        'channels': audio.channels,
        'bit_depth': audio.sample_width * 8,
        'file_format': file_format
    }

# ==============================================================================
# 4. ä¸¥æ ¼ç›¸å¯¹ä½ç½®æ¨¡å¼çš„é€Ÿåº¦è®¡ç®—
# ==============================================================================

def calculate_strict_position_speeds(
    segments: List[Dict],
    durations: Dict[str, float],
    base_rate: float,
    high_density_factor: float,
    low_density_factor: float,
    total_duration_ms: float
) -> Dict[str, float]:
    """
    è®¡ç®—ä¸¥æ ¼ç›¸å¯¹ä½ç½®æ¨¡å¼ä¸‹çš„å„ç±»å‹é€Ÿåº¦
    
    æ ¸å¿ƒåŸåˆ™ï¼š
    1. è¯­éŸ³ç‰‡çš„ä¸­ç‚¹ä½ç½®å¿…é¡»ä¸¥æ ¼æŒ‰å€é€Ÿç¼©æ”¾ï¼ˆåŸä½ç½®/å€é€Ÿï¼‰
    2. å…è®¸è¯­éŸ³ç‰‡ä»å‰åé™éŸ³"å€Ÿç”¨"ä¸€ç‚¹æ—¶é—´ï¼Œè®©è¯­éŸ³æ’­æ”¾å¾—ç¨æ…¢
    3. æ•´ä½“æ—¶é•¿å¿…é¡»ç²¾ç¡®ç­‰äº åŸæ—¶é•¿/å€é€Ÿ
    
    å®ç°ç­–ç•¥ï¼š
    - é«˜å¯†åº¦è¯­éŸ³é€Ÿåº¦ = base_rate * high_density_factorï¼ˆfactor < 1 æ—¶è¯­éŸ³å˜æ…¢ï¼‰
    - ä½å¯†åº¦è¯­éŸ³é€Ÿåº¦ = base_rate * low_density_factor
    - é™éŸ³é€Ÿåº¦åŠ¨æ€è®¡ç®—ï¼Œè¡¥å¿è¯­éŸ³"å€Ÿç”¨"çš„æ—¶é—´
    
    Args:
        segments: ç‰‡æ®µåˆ—è¡¨
        durations: å„ç±»å‹ç‰‡æ®µçš„æ€»æ—¶é•¿
        base_rate: åŸºå‡†å€é€Ÿ
        high_density_factor: é«˜å¯†åº¦å› å­ï¼ˆä¸¥æ ¼æ¨¡å¼ä¸‹æ¥è¿‘1.0ï¼Œä½†å¯ä»¥ç¨å°ä»¥ä¿æŠ¤è¯­éŸ³ï¼‰
        low_density_factor: ä½å¯†åº¦å› å­ï¼ˆä¸¥æ ¼æ¨¡å¼ä¸‹æ¥è¿‘1.0ï¼Œä½†å¯ä»¥ç¨å¤§ä»¥å‹ç¼©åœé¡¿ï¼‰
        total_duration_ms: åŸå§‹æ€»æ—¶é•¿
    
    Returns:
        å„ç±»å‹çš„é€Ÿåº¦å­—å…¸
    """
    target_total_duration = total_duration_ms / base_rate
    
    # è®¡ç®—è¯­éŸ³éƒ¨åˆ†çš„é€Ÿåº¦
    # åœ¨ä¸¥æ ¼æ¨¡å¼ä¸‹ï¼Œfactor æ¥è¿‘ 1.0ï¼Œä½†å…è®¸å°å¹…è°ƒæ•´
    speed_high = base_rate * high_density_factor
    speed_low = base_rate * low_density_factor
    
    # è®¡ç®—è¯­éŸ³éƒ¨åˆ†å˜é€Ÿåçš„æ—¶é•¿
    duration_high_after = durations['high_density_speech'] / speed_high if speed_high > 0 else 0
    duration_low_after = durations['low_density_speech'] / speed_low if speed_low > 0 else 0
    
    # è®¡ç®—è¯­éŸ³éƒ¨åˆ†"å€Ÿç”¨"æˆ–"èŠ‚çœ"çš„æ—¶é—´
    # å¦‚æœ factor < 1ï¼Œè¯­éŸ³å˜æ…¢ï¼Œéœ€è¦ä»é™éŸ³å€Ÿç”¨æ—¶é—´
    # å¦‚æœ factor > 1ï¼Œè¯­éŸ³å˜å¿«ï¼Œå¯ä»¥ç»™é™éŸ³æ›´å¤šæ—¶é—´
    ideal_high_after = durations['high_density_speech'] / base_rate
    ideal_low_after = durations['low_density_speech'] / base_rate
    
    time_borrowed = (duration_high_after - ideal_high_after) + (duration_low_after - ideal_low_after)
    
    # é™éŸ³éœ€è¦è¡¥å¿å€Ÿç”¨çš„æ—¶é—´
    ideal_silence_after = durations['silence'] / base_rate
    actual_silence_after = ideal_silence_after - time_borrowed
    
    if actual_silence_after <= 0 or durations['silence'] == 0:
        # æ²¡æœ‰è¶³å¤Ÿçš„é™éŸ³æ¥è¡¥å¿ï¼Œéœ€è¦æåº¦å‹ç¼©é™éŸ³
        speed_silence = 100.0
        print("è­¦å‘Š: é™éŸ³ä¸è¶³ä»¥è¡¥å¿è¯­éŸ³å€Ÿç”¨çš„æ—¶é—´ï¼Œé™éŸ³å°†è¢«æåº¦å‹ç¼©ã€‚")
        print(f"  - è¯­éŸ³å€Ÿç”¨æ—¶é—´: {time_borrowed:.2f}ms")
        print(f"  - ç†æƒ³é™éŸ³æ—¶é•¿: {ideal_silence_after:.2f}ms")
    else:
        speed_silence = durations['silence'] / actual_silence_after
    
    # éªŒè¯æ€»æ—¶é•¿
    total_after = duration_high_after + duration_low_after + (durations['silence'] / speed_silence if speed_silence < 100 else 0)
    
    print(f"\nã€ä¸¥æ ¼ç›¸å¯¹ä½ç½®æ¨¡å¼ - é€Ÿåº¦è®¡ç®—ã€‘")
    print(f"  ç›®æ ‡æ€»æ—¶é•¿: {target_total_duration:.2f}ms")
    print(f"  è¯­éŸ³å€Ÿç”¨æ—¶é—´: {time_borrowed:.2f}ms ({'å€Ÿç”¨' if time_borrowed > 0 else 'èŠ‚çœ'})")
    print(f"  é«˜å¯†åº¦è¯­éŸ³: {durations['high_density_speech']:.0f}ms â†’ {duration_high_after:.0f}ms (é€Ÿåº¦ {speed_high:.2f}x)")
    print(f"  ä½å¯†åº¦è¯­éŸ³: {durations['low_density_speech']:.0f}ms â†’ {duration_low_after:.0f}ms (é€Ÿåº¦ {speed_low:.2f}x)")
    print(f"  é™éŸ³: {durations['silence']:.0f}ms â†’ {durations['silence']/speed_silence:.0f}ms (é€Ÿåº¦ {speed_silence:.2f}x)")
    
    return {
        'high_density_speech': speed_high,
        'low_density_speech': speed_low,
        'silence': speed_silence
    }

# ==============================================================================
# 5. æ ¸å¿ƒç®—æ³• v3ï¼šæ”¯æŒç«‹ä½“å£°å’Œä¸¥æ ¼ç›¸å¯¹ä½ç½®æ¨¡å¼
# ==============================================================================

def intelligent_speed_up_v3(
    audio_path: str,
    output_path: str,
    base_rate: float,
    high_density_factor: float,
    low_density_factor: float,
    silence_threshold: float = 0.2,
    low_density_threshold: float = 0.7,
    strict_position: bool = False,
    quality_config: Optional[AudioQualityConfig] = None
) -> Dict:
    """
    å¯¹éŸ³é¢‘æ–‡ä»¶è¿›è¡Œå¸¦è¯­éŸ³å¯†åº¦æ„ŸçŸ¥çš„æ™ºèƒ½å˜é€Ÿå¤„ç†ï¼ˆv3 ç‰ˆæœ¬ï¼‰ã€‚
    
    æ–°å¢åŠŸèƒ½ï¼š
    - ç«‹ä½“å£°æ”¯æŒï¼šä¿æŒåŸå§‹å£°é“æ•°
    - ä¸¥æ ¼ç›¸å¯¹ä½ç½®æ¨¡å¼ï¼šä¿æŒè¯­éŸ³ç‰‡æ®µçš„ç²¾ç¡®ç›¸å¯¹ä½ç½®ï¼ŒåŒæ—¶å…è®¸ä»é™éŸ³å€Ÿç”¨æ—¶é—´ä¼˜åŒ–å¬æ„Ÿ
    - éŸ³é¢‘è´¨é‡é…ç½®ï¼šè‡ªå®šä¹‰é‡‡æ ·ç‡ã€ä½æ·±ã€æ ¼å¼

    Args:
        audio_path (str): è¾“å…¥éŸ³é¢‘æ–‡ä»¶è·¯å¾„ã€‚
        output_path (str): è¾“å‡ºæ–‡ä»¶è·¯å¾„ã€‚
        base_rate (float): åŸºå‡†å€é€Ÿã€‚
        high_density_factor (float): é«˜å¯†åº¦è¯­éŸ³çš„é€Ÿåº¦è°ƒèŠ‚å› å­ã€‚
        low_density_factor (float): ä½å¯†åº¦è¯­éŸ³çš„é€Ÿåº¦è°ƒèŠ‚å› å­ã€‚
        silence_threshold (float): VADæ¦‚ç‡ä½äºæ­¤å€¼ä¸ºé™éŸ³ã€‚
        low_density_threshold (float): VADæ¦‚ç‡ä½äºæ­¤å€¼ä¸ºä½å¯†åº¦è¯­éŸ³ã€‚
        strict_position (bool): æ˜¯å¦å¯ç”¨ä¸¥æ ¼ç›¸å¯¹ä½ç½®æ¨¡å¼ã€‚
        quality_config (AudioQualityConfig): éŸ³é¢‘è´¨é‡é…ç½®ï¼ŒNone ä½¿ç”¨é»˜è®¤ã€‚

    Returns:
        Dict: åŒ…å«å¤„ç†ç»“æœä¿¡æ¯çš„å­—å…¸
    """
    start_time = time.time()
    print("å¼€å§‹åŠ è½½æ¨¡å‹å’ŒéŸ³é¢‘ (v3)...")

    try:
        model, _ = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=False)
        original_audio = AudioSegment.from_file(audio_path)
    except Exception as e:
        print(f"åˆå§‹åŒ–å¤±è´¥: {e}")
        raise
    
    # é»˜è®¤è´¨é‡é…ç½®
    if quality_config is None:
        quality_config = AudioQualityConfig.quick_preview()
    
    # ä¿å­˜åŸå§‹éŸ³é¢‘å‚æ•°
    original_frame_rate = original_audio.frame_rate
    original_channels = original_audio.channels
    original_sample_width = original_audio.sample_width
    
    total_duration_ms = len(original_audio)
    
    print(f"éŸ³é¢‘åŠ è½½å®Œæˆï¼Œæ€»æ—¶é•¿: {total_duration_ms / 1000:.2f} ç§’")
    print(f"åŸå§‹å‚æ•°: {original_frame_rate}Hz, {original_channels}å£°é“, {original_sample_width * 8}bit")
    print(f"ä¸¥æ ¼ç›¸å¯¹ä½ç½®æ¨¡å¼: {'å¯ç”¨' if strict_position else 'ç¦ç”¨'}")

    print("\n--- æ­¥éª¤ 1: VADæ¦‚ç‡åˆ†æ ---")
    speech_probabilities = get_speech_probabilities(audio_path, model)
    if not speech_probabilities:
        print("æ— æ³•åˆ†æéŸ³é¢‘æ¦‚ç‡ï¼Œå·²é€€å‡ºã€‚")
        raise ValueError("VADåˆ†æå¤±è´¥")

    print("\n--- æ­¥éª¤ 2: è®¡ç®—å¤šçº§å˜é€Ÿé€Ÿç‡ ---")
    segments = create_multi_level_segments(total_duration_ms, speech_probabilities, silence_threshold, low_density_threshold)
    
    durations = {'silence': 0, 'low_density_speech': 0, 'high_density_speech': 0}
    for s in segments:
        durations[s['type']] += (s['end'] - s['start'])

    # æ ¹æ®æ¨¡å¼è®¡ç®—é€Ÿåº¦
    if strict_position:
        # ä¸¥æ ¼ç›¸å¯¹ä½ç½®æ¨¡å¼ï¼šå…è®¸ä»é™éŸ³å€Ÿç”¨æ—¶é—´
        speeds = calculate_strict_position_speeds(
            segments=segments,
            durations=durations,
            base_rate=base_rate,
            high_density_factor=high_density_factor,
            low_density_factor=low_density_factor,
            total_duration_ms=total_duration_ms
        )
        speed_high = speeds['high_density_speech']
        speed_low = speeds['low_density_speech']
        speed_silence = speeds['silence']
    else:
        # æ™®é€šæ¨¡å¼ï¼šå·®å¼‚åŒ–å˜é€Ÿï¼Œé™éŸ³åŠ¨æ€å‹ç¼©
        speed_high = base_rate * high_density_factor
        speed_low = base_rate * low_density_factor
        
        # æ ¹æ®æ€»æ—¶é•¿çº¦æŸï¼Œåæ¨é™éŸ³é€Ÿåº¦
        target_total_duration = total_duration_ms / base_rate
        duration_after_speech_processing = (durations['high_density_speech'] / speed_high) + \
                                           (durations['low_density_speech'] / speed_low)
        
        remaining_duration_for_silence = target_total_duration - duration_after_speech_processing
        
        if remaining_duration_for_silence <= 0 or durations['silence'] == 0:
            speed_silence = 100.0
            print("è­¦å‘Š: äººå£°éƒ¨åˆ†å˜é€Ÿåå·²è¶…è¿‡ç›®æ ‡æ€»æ—¶é•¿ï¼Œé™éŸ³å°†è¢«æåº¦å‹ç¼©ã€‚")
        else:
            speed_silence = durations['silence'] / remaining_duration_for_silence
        
        speeds = {
            'high_density_speech': speed_high,
            'low_density_speech': speed_low,
            'silence': speed_silence
        }
    
    print(f"\nåŸºå‡†å€é€Ÿ: {base_rate:.2f}x")
    print(f"é«˜å¯†åº¦è¯­éŸ³é€Ÿåº¦: {speed_high:.2f}x (å› å­: {high_density_factor})")
    print(f"ä½å¯†åº¦è¯­éŸ³é€Ÿåº¦: {speed_low:.2f}x (å› å­: {low_density_factor})")
    print(f"è®¡ç®—å¾—å‡ºçš„é™éŸ³é€Ÿåº¦: {speed_silence:.2f}x")

    print("\n--- æ­¥éª¤ 3: åˆ†æ®µå˜é€Ÿä¸åˆå¹¶ ---")
    
    # ç¡®å®šå¤„ç†æ—¶ä½¿ç”¨çš„éŸ³é¢‘ï¼ˆä¿æŒåŸå§‹å£°é“æˆ–è½¬å•å£°é“ï¼‰
    preserve_channels = quality_config.preserve_channels
    if preserve_channels and original_channels > 1:
        print(f"ä¿æŒç«‹ä½“å£°å¤„ç† ({original_channels} å£°é“)")
        audio_to_process = original_audio
    else:
        print("ä½¿ç”¨å•å£°é“å¤„ç†")
        audio_to_process = original_audio.set_channels(1)
    
    processed_segments = []
    for i, segment_info in enumerate(segments):
        start_ms, end_ms = segment_info['start'], segment_info['end']
        seg_type = segment_info['type']
        
        if start_ms >= end_ms: 
            continue

        original_chunk = audio_to_process[start_ms:end_ms]
        speed = speeds[seg_type]
        
        # å¤„ç†éŸ³é¢‘ç‰‡æ®µ
        processed_chunk = _process_audio_chunk(original_chunk, speed)
        processed_segments.append(processed_chunk)
        
        if (i + 1) % 20 == 0 or i == len(segments) - 1:
            print(f"å¤„ç†è¿›åº¦: {i+1}/{len(segments)} ç‰‡æ®µ")

    final_audio = sum(processed_segments, AudioSegment.empty())
    
    print("\n--- æ­¥éª¤ 4: åº”ç”¨éŸ³é¢‘è´¨é‡é…ç½®å¹¶å¯¼å‡º ---")
    
    # è®¾ç½®é‡‡æ ·ç‡
    target_sample_rate = quality_config.sample_rate or original_frame_rate
    if final_audio.frame_rate != target_sample_rate:
        final_audio = final_audio.set_frame_rate(target_sample_rate)
        print(f"é‡‡æ ·ç‡è°ƒæ•´ä¸º: {target_sample_rate}Hz")
    
    # è®¾ç½®ä½æ·±
    target_bit_depth = quality_config.bit_depth or (original_sample_width * 8)
    target_sample_width = target_bit_depth // 8
    if final_audio.sample_width != target_sample_width:
        final_audio = final_audio.set_sample_width(target_sample_width)
        print(f"ä½æ·±è°ƒæ•´ä¸º: {target_bit_depth}bit")
    
    # ç¡®å®šè¾“å‡ºæ ¼å¼å’Œå‚æ•°
    output_format = quality_config.output_format
    export_params = {}
    
    if output_format == "mp3":
        export_params["bitrate"] = quality_config.mp3_bitrate
        print(f"MP3 ç ç‡: {quality_config.mp3_bitrate}")
    
    # è°ƒæ•´è¾“å‡ºè·¯å¾„çš„æ‰©å±•å
    output_base = os.path.splitext(output_path)[0]
    output_path = f"{output_base}.{output_format}"
    
    final_audio.export(output_path, format=output_format, **export_params)
    
    end_time = time.time()
    original_duration = total_duration_ms / 1000
    final_duration = len(final_audio) / 1000
    
    print("\nğŸ‰ å¤„ç†å®Œæˆ (v3)ï¼")
    print(f"æ–‡ä»¶å·²ä¿å­˜è‡³: {output_path}")
    print(f"æ€»è€—æ—¶: {end_time - start_time:.2f} ç§’")
    print(f"åŸå§‹éŸ³é¢‘æ—¶é•¿: {original_duration:.2f} ç§’")
    print(f"è¾“å‡ºéŸ³é¢‘æ—¶é•¿: {final_duration:.2f} ç§’")
    print(f"å®ç°çš„å¹³å‡å€é€Ÿ: {original_duration / final_duration:.2f}x (ç›®æ ‡: {base_rate:.2f}x)")
    
    return {
        'success': True,
        'output_path': output_path,
        'original_duration': original_duration,
        'output_duration': final_duration,
        'actual_speed': original_duration / final_duration,
        'target_speed': base_rate,
        'processing_time': end_time - start_time,
        'segments_count': len(segments),
        'output_format': output_format,
        'output_sample_rate': target_sample_rate,
        'output_bit_depth': target_bit_depth,
        'output_channels': final_audio.channels,
        'strict_position': strict_position
    }


def _process_audio_chunk(chunk: AudioSegment, speed: float) -> AudioSegment:
    """
    å¯¹å•ä¸ªéŸ³é¢‘ç‰‡æ®µè¿›è¡Œå˜é€Ÿå¤„ç†ï¼Œæ”¯æŒç«‹ä½“å£°
    
    Args:
        chunk: éŸ³é¢‘ç‰‡æ®µ
        speed: å˜é€Ÿå€ç‡
    
    Returns:
        å¤„ç†åçš„éŸ³é¢‘ç‰‡æ®µ
    """
    if abs(speed - 1.0) < 0.01:
        return chunk
    
    # è·å–éŸ³é¢‘å‚æ•°
    channels = chunk.channels
    frame_rate = chunk.frame_rate
    sample_width = chunk.sample_width
    
    # è½¬æ¢ä¸º numpy æ•°ç»„
    samples = np.array(chunk.get_array_of_samples())
    
    # ç¡®å®šæ•°æ®ç±»å‹
    if sample_width == 1:
        dtype = np.int8
    elif sample_width == 2:
        dtype = np.int16
    elif sample_width == 4:
        dtype = np.int32
    else:
        dtype = np.int16
    
    samples = samples.astype(np.float64)
    
    if channels == 2:
        # ç«‹ä½“å£°ï¼šåˆ†ç¦»å·¦å³å£°é“
        samples = samples.reshape((-1, 2))
        left_channel = samples[:, 0]
        right_channel = samples[:, 1]
        
        # åˆ†åˆ«å¤„ç†æ¯ä¸ªå£°é“
        left_stretched = pyrb.time_stretch(left_channel, frame_rate, speed)
        right_stretched = pyrb.time_stretch(right_channel, frame_rate, speed)
        
        # ç¡®ä¿ä¸¤ä¸ªå£°é“é•¿åº¦ç›¸åŒ
        min_len = min(len(left_stretched), len(right_stretched))
        left_stretched = left_stretched[:min_len]
        right_stretched = right_stretched[:min_len]
        
        # åˆå¹¶å£°é“
        processed_samples = np.column_stack((left_stretched, right_stretched)).flatten()
    else:
        # å•å£°é“
        processed_samples = pyrb.time_stretch(samples, frame_rate, speed)
    
    # è½¬æ¢å›æ•´æ•°ç±»å‹
    max_val = 2 ** (sample_width * 8 - 1) - 1
    processed_samples = np.clip(processed_samples, -max_val, max_val).astype(dtype)
    
    # åˆ›å»ºæ–°çš„éŸ³é¢‘ç‰‡æ®µ
    processed_chunk = AudioSegment(
        processed_samples.tobytes(),
        frame_rate=frame_rate,
        sample_width=sample_width,
        channels=channels
    )
    
    return processed_chunk


# ==============================================================================
# 6. å‘åå…¼å®¹ï¼šä¿ç•™ v2 æ¥å£
# ==============================================================================

def intelligent_speed_up_v2(
    audio_path: str,
    output_path: str,
    base_rate: float,
    high_density_factor: float,
    low_density_factor: float,
    silence_threshold: float = 0.2,
    low_density_threshold: float = 0.7
):
    """
    å‘åå…¼å®¹çš„ v2 æ¥å£ï¼Œå†…éƒ¨è°ƒç”¨ v3 å®ç°
    """
    result = intelligent_speed_up_v3(
        audio_path=audio_path,
        output_path=output_path,
        base_rate=base_rate,
        high_density_factor=high_density_factor,
        low_density_factor=low_density_factor,
        silence_threshold=silence_threshold,
        low_density_threshold=low_density_threshold,
        strict_position=False,
        quality_config=AudioQualityConfig.quick_preview()
    )
    return result
